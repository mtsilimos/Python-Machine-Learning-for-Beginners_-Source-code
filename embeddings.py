# -*- coding: utf-8 -*-
"""Embeddings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n1bxfv9PTQ220oWj8FpfgxEJg7kJuHXm

**Word2Vec**
"""

!pip install gensim nltk

import nltk
nltk.download('punkt_tab')
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize

# Sample corpus
documents = ["This is a sentence", "Word embeddings capture meaning", "Text representation is important"]

# tokenize sentences, create a list of lists, where each inner list contains the tokens of a document
tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]

# Train Word2Vec model
# vector_size = 100: This specifies the dimensionality of the word vectors (embeddings). Each word will be represented by a vector of 100 numbers.
# window = 5: This defines the maximum distance between the current word and the words in its context. For example, if window=5, the model considers words within 5 words to the left and 5 words to the right of the target word when learning its embedding.
# min_count = 1: This ignores all words with a total frequency lower than this value.
# workers=4: This specifies the number of CPU cores to use for training. More workers can speed up the training process.

model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=1, workers=4)

# Get embedding for a word. This line retrieves the 100-dimensional vector representation for the word 'word'.
word_vector = model.wv['word']
print(word_vector)

"""**FastText**"""

from gensim.models import FastText

# Train FastText model
fasttext_model = FastText(sentences=tokenized_docs, vector_size=100, window=5, min_count=1, workers=4)

# Get embedding for a word
fasttext_vector = fasttext_model.wv['word']
print(fasttext_vector)

"""**GloVe**"""

import gensim.downloader as api

# Load pretrained GloVe embeddings
glove_model = api.load("glove-wiki-gigaword-100")

# Get embedding for a word
glove_vector = glove_model['word']
print(glove_vector)

"""**BERT Embeddings**"""

from transformers import BertTokenizer, BertModel
import torch

# Load pre-trained BERT model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# Tokenize and convert to tensor
text = "Text embeddings are powerful"

# return_tensors='pt': This tells the tokenizer to return the output as PyTorch tensors. A tensor of numerical IDs corresponding to each token in the input text.
inputs = tokenizer(text, return_tensors='pt')

# This line below feeds the tokenized input to the pre-trained BERT model; the **inputs syntax unpacks the inputs dictionary so that its keys are passed as keyword arguments to the model.
outputs = model(**inputs)

# Extract embedding
# This gives you a vector (768 numbers) for each token in your input sentence.
#.mean(dim=1): collapse the multiple token embeddings of a sentence into one single, comprehensive vector representing that entire sentence
#.detach: in order not to store the computational graph history
# .numpy(): convert a tensor to a numpy array
# The last_hidden_state is the final and most refined set of contextualized numerical representations that a BERT (or similar transformer encoder) model produces for each token in your input sequence.

bert_embedding = outputs.last_hidden_state.mean(dim=1).detach().numpy()
print(bert_embedding)

"""The embeddings from Word2Vec, FastText, GloVe and BERT are all dense, continuous vectors that encode semantic relationships, not raw counts or frequencies. Therefore, they are incompatible with Naive Bayes, which is a count-based probabilistic model. While these embeddings can technically be used as input features for a Random Forest model, Random Forest is generally not the optimal classifier for leveraging the high-dimensional, semantic information within these embeddings, and you'll typically achieve better results with models like Logistic Regression or Support Vector Machines (SVM)."""