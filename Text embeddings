1. Word2Vec (Using Gensim)

from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize

# Sample corpus
documents = ["This is a sentence", "Word embeddings capture meaning", "Text representation is important"]

# tokenize sentences, create a list of lists, where each inner list contains the tokens of a document
tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]

# Train Word2Vec model
# vector_size = 100: This specifies the dimensionality of the word vectors (embeddings). Each word will be represented by a vector of 100 numbers.
# window = 5: This defines the maximum distance between the current word and the words in its context. For example, if window=5, the model considers words within 5 words to the left and 5 words to the right of the target word when learning its embedding.
# min_count = 1: This ignores all words with a total frequency lower than this value. 
# workers=4: This specifies the number of CPU cores to use for training. More workers can speed up the training process.

model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=1, workers=4)

# Get embedding for a word. This line retrieves the 100-dimensional vector representation for the word 'word'.
word_vector = model.wv['word']
print(word_vector)


2. FastText (Using Gensim)

from gensim.models import FastText

# Train FastText model
fasttext_model = FastText(sentences=tokenized_docs, vector_size=100, window=5, min_count=1, workers=4)

# Get embedding for a word
fasttext_vector = fasttext_model.wv['word']
print(fasttext_vector)


3. GloVe (Using Pretrained Vectors)

import gensim.downloader as api

# Load pretrained GloVe embeddings
glove_model = api.load("glove-wiki-gigaword-100")

# Get embedding for a word
glove_vector = glove_model['word']
print(glove_vector)


4. BERT Embeddings (Using Transformers)

from transformers import BertTokenizer, BertModel
import torch

# Load pre-trained BERT model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# Tokenize and convert to tensor
text = "Text embeddings are powerful"

# return_tensors='pt': This tells the tokenizer to return the output as PyTorch tensors. A tensor of numerical IDs corresponding to each token in the input text.
inputs = tokenizer(text, return_tensors='pt')

# This line below feeds the tokenized input to the pre-trained BERT model; the **inputs syntax unpacks the inputs dictionary so that its keys are passed as keyword arguments to the model.
outputs = model(**inputs)

# Extract embedding
# This gives you a vector (768 numbers) for each token in your input sentence.
#.mean(dim=1): collapse the multiple token embeddings of a sentence into one single, comprehensive vector representing that entire sentence
#.detach: in order not to store the computational graph history 
# .numpy(): convert a tensor to a numpy array
# The last_hidden_state is the final and most refined set of contextualized numerical representations that a BERT (or similar transformer encoder) model produces for each token in your input sequence.

bert_embedding = outputs.last_hidden_state.mean(dim=1).detach().numpy()
print(bert_embedding)


REFERENCE
KoshurAI: The Ultimate Guide to Text Embeddings: Understanding and Generating Them in Python 
Link:https://medium.com/aimonks/the-ultimate-guide-to-text-embeddings-understanding-and-generating-them-in-python-c30795210238
